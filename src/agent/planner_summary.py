import os
import sys
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
import base64
from io import BytesIO
import concurrent.futures
import hashlib
import json
from tqdm import tqdm
import logging

logger = logging.getLogger('planner_summary')

from .editing_utils import load_video_footages
from .llm_interface import chat_with_llm, Message
from ..features import VideoFeatures
from ..utils.path import get_output_dir

summary_dir = get_output_dir() / "summary"
os.makedirs(summary_dir, exist_ok=True)


MAX_WORKERS = 8

def pil_to_base64(image, format="JPEG", quality=85):
    """
    å°† PIL å›¾ç‰‡å¯¹è±¡è½¬æ¢ä¸º base64 å­—ç¬¦ä¸²
    :param image: PIL.Image å¯¹è±¡
    :param format: ä¿å­˜æ ¼å¼ï¼ˆJPEG å¯å‡å°ä½“ç§¯ï¼ŒPNG è´¨é‡æ›´é«˜ï¼‰
    :param quality: JPEG å‹ç¼©è´¨é‡ (1-100)
    """
    buffered = BytesIO()
    # å°†å›¾ç‰‡ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº
    image.save(buffered, format=format, quality=quality)
    # è·å–å­—èŠ‚æ•°æ®å¹¶è¿›è¡Œ base64 ç¼–ç 
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    return img_str

class OpenCVCinematicIterator:
    def __init__(self, video_paths: dict[str, Path], video_features: dict[str, VideoFeatures], batch_size=16, slot_width=448, slot_height=252):
        """
        :param slot_width: æ¯ä¸ªå°æ ¼å­çš„å®½åº¦ (é»˜è®¤ 448)
        :param slot_height: æ¯ä¸ªå°æ ¼å­çš„é«˜åº¦ (å»ºè®®è®¾ä¸º 16:9 æ¯”ä¾‹ï¼Œå¦‚ 252)
        """
        self.video_paths = video_paths
        self.batch_size = batch_size
        self.per_batch = batch_size
        self.slot_size = (slot_width, slot_height)
        
        # æ‘Šå¹³æ‰€æœ‰ shot
        self.flat_shots = []
        for vid, feature in video_features.items():
            if vid in video_paths:
                for shot_idx, shot in enumerate(feature.shots):
                    self.flat_shots.append({
                        'vid': vid,
                        'path': str(video_paths[vid]),
                        'start': shot['start'],
                        'end': shot['end'],
                        'idx': shot_idx
                    })
        
    def _get_letterboxed_frame(self, frame, target_size):
        """ä¿æŒæ¯”ä¾‹ç¼©æ”¾å¹¶å¡«å……é»‘è¾¹"""
        t_w, t_h = target_size
        h, w = frame.shape[:2]
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale = min(t_w / w, t_h / h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        # ç¼©æ”¾
        resized = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # åˆ›å»ºé»‘è‰²ç”»å¸ƒå¹¶å±…ä¸­ç²˜è´´
        canvas = np.zeros((t_h, t_w, 3), dtype=np.uint8)
        x_offset = (t_w - new_w) // 2
        y_offset = (t_h - new_h) // 2
        canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
        return canvas

    def __iter__(self):
        cap = None
        current_path = None
        
        # æŒ‰ batch å¤„ç†
        for i in range(0, len(self.flat_shots), self.per_batch):
            batch = self.flat_shots[i : i + self.per_batch]
            processed_frames = []
            
            for shot in batch:
                # ä¼˜åŒ–ï¼šé¿å…é‡å¤æ‰“å¼€åŒä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                if shot['path'] != current_path:
                    if cap is not None:
                        cap.release()
                    cap = cv2.VideoCapture(shot['path'])
                    current_path = shot['path']
                
                # å–ä¸­é—´å¸§
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_pos = (shot['start'] + shot['end']) // 2
                
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)
                ret, frame = cap.read()
                
                if ret:
                    # è½¬æ¢é¢œè‰²ç©ºé—´ (OpenCV æ˜¯ BGR, VLM é€šå¸¸éœ€è¦ RGB)
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    processed_frames.append(self._get_letterboxed_frame(frame, self.slot_size))
                else:
                    processed_frames.append(np.zeros((self.slot_size[1], self.slot_size[0], 3), dtype=np.uint8))

            yield processed_frames, batch

        if cap is not None:
            cap.release()

def _get_batch_summary_prompt(genre: str, batch_size: int):
    return f"""
### Task
Act as a Mashup Editor. Scan these {batch_size} shots from a {genre} library to identify high-value editing assets.

### Instructions
1. **Identify "Edit Hooks"**: Look for shots with strong motion (zooms, pans), high emotional intensity (eyes, screams), or striking aesthetics (neon, shadows).
2. **Action & Rhythm**: Focus on the kinetic energy. Is it fast-paced, slow-motion, or rhythmic?
3. **Ignore Metadata**: Strictly ignore UI overlays (#1-#16) and grid lines. Focus on the raw footage.
4. **Brevity**: Use punchy, descriptive tags that an editor can search for.

### Output Format
- **Visual Vibe**: [3 keywords for the aesthetic, e.g., Glitchy, Epic, Gritty]
- **Key Assets**: [Main subjects/actions, e.g., Car drifting, Sword clash, Rain-slicked face]
- **Mashup Potential**: [Describe the best 1-2 shots for a music video or trailerâ€”focus on "The Money Shot"]
- **Rhythm**: [Describe the motion speed, e.g., Fast cuts, Slow-burn, Explosive]
"""

def _get_aggregation_prompt(full_corpus: str, total_shots: int, genre: str):
    return f"""
### Role
You are a Lead Mashup Strategist. Synthesize the provided logs from {total_shots} shots into a professional "Footage Analysis Report" for creative editing.

### Source Data
Below is the raw chronological narrative log of the footage from the {genre} project:
---
{full_corpus}
---

### Objective
Synthesize the data above into a strategic blueprint for a mashup video.
**Constraint**: Eliminate all technical indices and segment IDs. The final report must flow like a creative pitch.

### Output Structure

**Footage Analysis Report: [Project Title]**

**1. Visual Identity & Aesthetic**
A paragraph describing the "Cinematic Look" (color, lighting, texture) and how it dictates the mashup's vibe.

**2. Global Keywords**
A comma-separated list of 15 high-impact keywords for asset retrieval.

**3. Iconic Scene Library (Archetypes)**
Group the most memorable footage into 4-6 categories. For each:
- **[Set Title]**: (e.g., The Kinetic Chase, The Neon Void)
- **Edit-Value**: Why these shots are perfect for a rhythmic or emotional hook.
- **Standout Moments**: Describe 2-3 specific, high-impact highlights found in the logs.

**4. Visual Anchors & Motifs**
Identify recurring symbols (e.g., a specific prop, color shift) that can serve as recurring transitions or thematic anchors.

### Style Guide
- Professional, high-energy, and editor-focused.
- Use cinematic terminology (e.g., "match-cuts," "low-key lighting").
"""
def process_task(batch_idx, batch_data, genre):
    """å¤„ç†ä¸€ä¸ª batch çš„å¤šå¼ å›¾å¹¶è¯·æ±‚ LLM"""
    frames, metadata = batch_data
    try:
        msg = Message(role="user")
        for frame in frames:
            frame_pil = Image.fromarray(frame)
            b64_str = pil_to_base64(frame_pil, quality=80)
            msg = msg.add_image_base64(b64_str)

        msg = msg.add_text(_get_batch_summary_prompt(genre=genre, batch_size=len(frames)))

        # è°ƒç”¨ä½ çš„ API æ¥å£
        res = chat_with_llm([msg.to_dict()])
        
        return {"id": batch_idx, "txt": res.strip()}
    except Exception as e:
        logger.error(f"Error processing batch {batch_idx}: {e}")
        return {"id": batch_idx, "error": str(e)}

def run_pipeline(video_paths, video_features, totshot, genre: str, max_workers: int=MAX_WORKERS):
    iterator = OpenCVCinematicIterator(video_paths, video_features)
    results = []

    # tqdm è¿›åº¦æ¡
    pbar = tqdm(total=(totshot + 15) // 16, desc="ğŸï¸ Analyzing Footage")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_id = {}

        for i, data in enumerate(iterator):
            future = executor.submit(process_task, i, data, genre)
            future_to_id[future] = i

            if len(future_to_id) >= max_workers * 2:
                done, _ = concurrent.futures.wait(
                    future_to_id.keys(),
                    return_when=concurrent.futures.FIRST_COMPLETED
                )
                for f_done in done:
                    result = f_done.result()
                    results.append(result)
                    del future_to_id[f_done]
                    pbar.update(1)

        for f_done in concurrent.futures.as_completed(future_to_id):
            result = f_done.result()
            results.append(result)
            pbar.update(1)

    pbar.close()
    
    return results

def get_summary(csv_path: Path, data_root: Path, output_root: Path, summary_root: Path = summary_dir, genre: str = "Video Collection", use_cache: bool = True) -> str:
    """
    ç”Ÿæˆè§†é¢‘ç´ æçš„ Planner Summary
    :param csv_path: åŒ…å«è§†é¢‘è·¯å¾„çš„ CSV æ–‡ä»¶
    :param data_root: è§†é¢‘æ–‡ä»¶çš„æ ¹ç›®å½•
    :param output_root: è¾“å‡ºç›®å½•
    :param summary_root: Summary ç¼“å­˜ç›®å½•
    :param genre: è§†é¢‘ç±»å‹
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    """
    summary_root.mkdir(parents=True, exist_ok=True)

    # è®¡ç®— CSV å†…å®¹ + genre çš„ Hashï¼Œç”¨äºç¼“å­˜æ ¡éªŒ
    csv_bytes = csv_path.read_bytes()
    cache_hash = hashlib.md5(csv_bytes + genre.encode("utf-8")).hexdigest()
    cache_path = summary_root / f"{cache_hash}.json"

    if cache_path.exists() and use_cache:
        try:
            cached = json.loads(cache_path.read_text(encoding="utf-8"))
            logger.info(f"Using cached summary from {cache_path}")
            return cached["summary"]
        except json.JSONDecodeError:
            pass
    logger.info("Generating new summary...")
    # é‡æ–°ç”Ÿæˆ summary
    video_paths, video_features = load_video_footages(csv_path, data_root, output_root)
    totshot = sum(len(video_feature.shots) for video_feature in video_features.values())

    results = run_pipeline(video_paths, video_features, totshot, genre)

    # æŒ‰ id æ’åºå¹¶æ‹¼æ¥æˆè¯­æ–™
    results = sorted(results, key=lambda x: x.get("id", 0))
    segments = []
    for item in results:
        if "txt" in item:
            segments.append(item['txt'])

    full_corpus = "\n".join(segments)
    aggregation_prompt = _get_aggregation_prompt(full_corpus, total_shots=totshot, genre=genre)
    summary = chat_with_llm([
        Message(role="user").add_text(aggregation_prompt).to_dict()
    ])

    cache_payload = {
        "csv_path": str(csv_path),
        "genre": genre,
        "cache_hash": cache_hash,
        "segments": results,
        "summary": summary
    }
    cache_path.write_text(json.dumps(cache_payload, ensure_ascii=False, indent=2), encoding="utf-8")

    return summary
    
